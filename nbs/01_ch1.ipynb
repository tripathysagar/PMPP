{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch1 : Introduction \n",
    "\n",
    "> GPU programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ch1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "  <span>ðŸ“… 09/12/2025</span>\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Modern computer system are build on the CPU which is central to all modern life. It takes in pragram and perform execution sequentially. CPU stopped  getting dramatically faster due to power and heat issue. But **Moore's law** continues and putting more transistor to the chip i.e. more cores. \n",
    "\n",
    "\n",
    "Enters parallelism with GPU, these are multiple small compute units. Where each unit is responsible for a single computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CPU Memory Layout**\n",
    "\n",
    "- **Large caches** â€” CPUs have big L1, L2, and L3 caches to hide memory latency\n",
    "- **Optimized for low latency** â€” designed to get *one* piece of data as fast as possible\n",
    "- **Cache-friendly access** â€” works well even with somewhat random access patterns because the cache \"saves\" recently used data\n",
    "\n",
    "## **GPU Memory Layout**\n",
    "\n",
    "- **Small caches per core** â€” each core has much less cache (thousands of cores sharing limited cache)\n",
    "- **Optimized for high throughput** â€” designed to deliver *lots* of data, not necessarily fast for any single request\n",
    "- **Global memory is slow** â€” the main GPU memory (VRAM) has high latency (hundreds of cycles)\n",
    "- **Coalesced access is critical** â€” when adjacent threads access adjacent memory locations, the GPU can combine these into one efficient transaction. Random access patterns kill performance.\n",
    "- **Shared memory** â€” a small, fast, programmer-controlled memory shared within a thread block (like a manual cache)\n",
    "\n",
    "\n",
    "CPUs *hide* memory latency with big caches. GPUs *tolerate* memory latency by having thousands of threads â€” while some threads wait for data, others execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many problems can benifit from parallelization. But there are many bottleneck one has to be aware of before writing efficient GPU code.\n",
    "1. Moving data back and forth CPU and GPU is time consuming\n",
    "1. **Amdahl's Law**:  not all parts of a program can be parallelized\n",
    "1. The sequential portion dominates the execution\n",
    "1. **Race conditions** : threads accessing same data unpredictably and having invalid data\n",
    "1. **Synchronization overhead** : coordinating threads takes time some threads are faster some might be slower\n",
    "1. **Load balancing** : all processors should have equal work\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Amdahl's Law\n",
    "\n",
    "**Amdahl's Law** tells us the **maximum speedup** achievable by parallelizing a program.\n",
    "\n",
    "Speedup = 1 / ((1 - P) + P / N)\n",
    "\n",
    "Where:\n",
    "- **P** = fraction of the program that *can* be parallelized\n",
    "- **N** = number of processors\n",
    "\n",
    "\n",
    "Imagine a program that takes **100 seconds** on one processor, and **80%** can be parallelized (P = 0.8).\n",
    "\n",
    "| Processors (N) | Calculation | Speedup | New Time |\n",
    "|----------------|-------------|---------|----------|\n",
    "| 1 | 1 / (0.2 + 0.8/1) | 1Ã— | 100s |\n",
    "| 4 | 1 / (0.2 + 0.8/4) | 2.5Ã— | 40s |\n",
    "| 16 | 1 / (0.2 + 0.8/16) | 4Ã— | 25s |\n",
    "| 100 | 1 / (0.2 + 0.8/100) | ~4.8Ã— | ~21s |\n",
    "| âˆž | 1 / (0.2 + 0) | 5Ã— | 20s |\n",
    "\n",
    "\n",
    "\n",
    "Even with **infinite processors**, the maximum speedup is only **5Ã—** because the sequential 20% (20 seconds) can never be parallelized.\n",
    "\n",
    "This is why minimizing sequential code is critical in GPU programming!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (manim_env)",
   "language": "python",
   "name": "manim_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
